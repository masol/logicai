

# LogicAI

**让LLM从直觉进化到逻辑，逻辑向工作流编排系统，自动创建可执行复杂任务的Agent**

[![Stars](https://img.shields.io/github/stars/masol/logicai?style=social)](https://github.com/masol/logicai/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.0.1-green.svg)](https://github.com/masol/logicai/releases)

[English](README.cn.md.md) |  **中文**

---
# 项目概述

本项目旨在构建一个基于逻辑推理的自动任务分解系统，通过将大语言模型（LLM）的调用视为幂等函数，实现复杂任务的分解与编排。系统的核心在于将LLM的直觉式推理能力与形式逻辑体系相结合，通过构建和维护一个本体空间，实现任务分解和编排的持续迭代。

在这个框架中，LLM的输入输出都被视为结构化数据，提示词的构成也遵循这一原则，不再视其为语义，而是逻辑维度的集合。例如，当我们改进提示词，要求"更通俗易懂"或"更专业"地书写时，这些要求被LogicAI视为在输入数据中增加了新的维度——具体体现为在结构化数据中增加"可读性级别"和"专业程度"等条目。这些属性名可以通过LLM来动态获取，相当于逻辑学中的类型升阶——就像问"苹果是什么？"->"苹果是水果"，进一步问"水果是什么？"->"水果是植物产品"，递归获得完整的分类链条：苹果→水果→植物产品→生物制品→物质。这种结构化的视角使系统能够引入逻辑系统来处理和优化系统工作流。

整个系统将基于逻辑的形式系统与基于神经元的LLM(视为直觉)相结合，把系统复杂度从LLM层面转移到架构层面，让每个进入LLM的子任务保持在可控的复杂度范围内，从而提高系统可应对的复杂度--将整个系统视为一个输入/输出，可视其为提升了LLM能力的一个"外部增强模型"。

# 核心概念解释

为更好理解本项目的设计理念，我们可以将其类比为生产线设计的知识体系：

**生产线设计类比**：想象一个工程师需要设计一条生产线来制造特定产品。在这个类比中：

- **工程师的知识体系** = 本体空间（类似Prolog知识库）
- **具体的生产线** = 工作流
- **生产线蓝图的存储** = Agent
- **生产线操作工人的操作手册** = 工作流使用者所需的知识

工程师脑中拥有关于工业设计、流程优化、设备选型等专业知识，这些知识构成了一个逻辑体系，能够回答"如何设计一条高效的生产线"这样的问题。当工程师运用这些知识设计出具体的生产线后，生产线操作工人只需要掌握操作手册即可，无需理解整个设计知识体系。

类似地，想象一个具备充分知识的Prolog程序，它能够基于逻辑规则自主"设计"出生产线。在我们的系统中：

**设计层知识**（本体空间）：

以茶几花卉选择为例：设计师在花卉市场通过"尺寸"、"颜色"、"维护性"等维度筛选合适花卉。对应地，我们可构建提示词让LLM来选择，通过逐步添加"尺寸为XXX"、"颜色为XXX"等约束条件，最终使LLM决策与设计师判断趋同。

这一例子展示了LogicAI的核心理念：将提示词从语义表达重新定义为维度组合的结构化数据。这些维度并非任意确定，而是从交付目标（花卉）需满足的现实约束（茶几适配性、用户偏好）出发，通过逻辑推演得出。

关键在于将逻辑体系构建任务明确分割：通过专门设计的元提示词，让LLM系统性地挖掘决策维度，自动推导约束条件与逻辑规则，最终构建完整的规则库。

**执行层知识**（工作流使用）：

对于住户而言，只要花卉能带来心情愉悦即可，无需掌握前述的设计层知识。换言之，固化后的工作流(Agent)能否满足需求，关键在于功能实现，而非实现方式——无论是人工设计还是AI自主生成。

然而，当住户因失眠希望通过花卉获得心情平静时，现有Agent需要改进。这种改进依赖设计层知识，需要Agent设计者更新系统。因此，当前上下文工程设计的Agent仅适用于应用场景相对稳定的情况，而在小说创作、编程开发等变化频繁的现实领域，难以满足动态需求。

进一步思考，如现实世界中，若花卉仅是住户为其用户提供产品的一个环节，住户本身也成为设计师，同样可采用上述方法处理。这构成了工作流之间的嵌套调用关系——当然，当前版本尚不支持此特性。


**与Chain of Thought（COT）的区别**：

- COT方法：通过"step by step"的提示词引导LLM进行线性思考
- 本方法：首先通过询问"实现XXX交付物的最佳实践步骤"获取初始工作流（利用LLM直觉），然后将工作流每个步骤的输入输出进行类型分析，构建分类树，递归探究其处理规则，形成逻辑世界。通过查询这个逻辑世界来分析和改进工作流，在此过程中，逻辑世界（本体空间）和工作流实现同步进化。

**与Reasoning + Acting（ReAct）的区别**：

- ReAct方法：ReAct方法采用推理(Reasoning) + 行动(Acting) + 观察(Observation)的AI问题解决模式，但其推理环节依赖人工设计，缺乏系统化的设计方法论。
- 本方法：本方法专注于推理环节的优化，通过算法化手段替代人工设计，实现推理过程的自动化构建。

**与Plan-and-Execute的区别**:

- Plan-and-Execute是LangChain框架中的经典模式，将复杂任务分解为规划和执行两个独立阶段，面临抽象层次与执行精度间的矛盾——高层规划可能丢失关键上下文，详细规划则可能牺牲系统灵活性。当前的claude-code、Cursor等自动开发工具的实践表明了这一挑战：即使将抽象层级聚焦在软件开发领域，仍会遭遇上下文丢失导致的代码架构不一致、模块接口冲突等问题。
- 本方法采用不同的设计思路：将LLM调用建模为幂等函数，通过逻辑系统维护输入输出间的维度关系，理论上可以在保持上下文完整性的同时确保系统灵活性，有望避免抽象与执行脱节的问题，使完整小说创作、大型软件开发等超大规模任务成为可能——参考下文的长远展望，了解更多可能性。

# 要解决的问题

## 1. LLM应用的复杂度控制

现有LLM在复杂任务中存在准确率下降、逻辑混乱等问题，而提示工程等优化方法仍受限于模型能力边界。

## 2. AI系统的不可解释性

大多数基于LLM的系统缺乏清晰的推理路径，难以追溯决策过程，这在需要高可靠性的应用场景中成为关键障碍。

## 3. 逻辑与直觉的割裂

LLM的直觉式推理与专家系统的形式逻辑推理缺乏有效桥梁。直觉推理擅长模式识别，逻辑推理确保严密性，但两者难以融合，限制了AI在复杂场景中的应用。

# 解决方案

## 1. 幂等函数化的LLM调用

将每次LLM调用封装为幂等函数，确保相同输入总是产生相同输出，使得系统行为可预测、可重现。这种设计让LLM调用变得类似传统程序中的纯函数，便于组合和测试。

## 2. 本体空间的构建与维护

系统维护一个类似Prolog知识库的逻辑空间，该空间包含设计工作流所需的核心知识：

- **知识树(分类树)**：将所有输入输出进入分类树，形成层次化的类型体系
- **逻辑规则库**：每种类型数据的处理规则、约束条件和转换规律
- **推理引擎**：能够基于已有规则推导出新的工作流设计方案
- **知识更新机制**：从工作流执行结果中提取新的规则和模式

## 3. 递归任务分解与逻辑构建

通过以下步骤实现从直觉到逻辑的进化：

1. **初始直觉获取**：向LLM询问最佳实践，获得初始工作流
2. **类型分析**：分析每个步骤的输入输出，构建类型分类树
3. **规则提取**：递归探究各类型的处理规律，形成逻辑规则
4. **逻辑查询**：通过查询逻辑世界分析工作流的合理性
5. **同步优化**：基于分析结果同时改进工作流和逻辑规则

## 4. 工作流持久化与Agent生成

将验证过的工作流持久化为Agent，每个Agent专注于解决特定类型的问题。使用者无需理解底层的设计逻辑，只需掌握Agent的使用方法即可。

# 技术目标

1. **逻辑可查询性**：构建类似Prolog的知识库，能够通过逻辑查询分析和优化工作流
    
2. **双体系协同**：设计知识与执行知识分离且相互促进，实现知识的层次化管理
    
3. **自进化能力**：本体空间和工作流在执行过程中持续学习和优化
    
4. **知识复用性**：抽象的逻辑规则可以指导多种相似工作流的设计
    

# 长远展望：自我进化的LogicAI

**核心洞察**：在我们的设计框架下，一切皆函数，一切皆工作流。目标判定本身、工作流设计过程、系统优化决策等，都可以被建模为函数，因此都可以被纳入本方法的持续迭代改进范围。

**自举基础环境**：我们提供的初始系统相当于一个具备自我更新能力的不动点——类似于数学中的巴拿赫不动点定理所描述的稳定状态，系统在这个基础上可以通过应用自身的逻辑推理能力来改进自己的推理能力。这种递归式的自我改进机制使得LogicAI能够在有限的初始投入下实现无限的成长潜力。

**元级工作流**：系统不仅能够分解用户任务，还能够分解"如何更好地分解任务"这一元任务，从而实现对自身能力的反思与提升。本体空间的逻辑规则也将包含关于知识获取、规则优化、推理改进等元认知规则。

**无边界成长**：通过这种设计，LogicAI理论上能够突破初始设计者的认知边界，在实践中发现更优的问题解决模式，并将这些发现编码为新的逻辑规则，形成真正的人工智能自主进化。

**当前阶段聚焦**：尽管长远目标如此宏大，限于精力和现实约束，当前版本将专注于复杂任务的自动拆解和实用Agent的生成。我相信，即使是这个相对有限的目标，也将为构建更强大的自进化AI系统奠定基础。

这一方法的目标是实现从LLM直觉向形式逻辑的进化，构建一个能够像人类工程师一样思考和设计的智能系统，并为未来的自主进化预留可能性。
