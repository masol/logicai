# LogicAI <!-- omit in toc -->

**让LLM从直觉进化到逻辑，逻辑向工作流编排系统，自动创建可执行复杂任务的Agent**

[![Stars](https://img.shields.io/github/stars/masol/logicai?style=social)](https://github.com/masol/logicai/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.0.1-green.svg)](https://github.com/masol/logicai/releases)

[English](../README.md) |  **中文**

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
## 目录 <!-- omit in toc -->

- [项目概述](#%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0)
- [概念解释](#%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A)
  - [生产线设计类比：](#%E7%94%9F%E4%BA%A7%E7%BA%BF%E8%AE%BE%E8%AE%A1%E7%B1%BB%E6%AF%94)
    - [设计层知识（本体空间）：](#%E8%AE%BE%E8%AE%A1%E5%B1%82%E7%9F%A5%E8%AF%86%E6%9C%AC%E4%BD%93%E7%A9%BA%E9%97%B4)
    - [执行层知识（工作流使用）：](#%E6%89%A7%E8%A1%8C%E5%B1%82%E7%9F%A5%E8%AF%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E4%BD%BF%E7%94%A8)
  - [与Chain of Thought（COT）的区别：](#%E4%B8%8Echain-of-thoughtcot%E7%9A%84%E5%8C%BA%E5%88%AB)
  - [与Reasoning + Acting（ReAct）的区别：](#%E4%B8%8Ereasoning--actingreact%E7%9A%84%E5%8C%BA%E5%88%AB)
  - [与Plan-and-Execute的区别:](#%E4%B8%8Eplan-and-execute%E7%9A%84%E5%8C%BA%E5%88%AB)
  - [与RAG的区别:](#%E4%B8%8Erag%E7%9A%84%E5%8C%BA%E5%88%AB)
- [要解决的问题](#%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98)
  - [1. LLM应用的复杂度控制](#1-llm%E5%BA%94%E7%94%A8%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%8E%A7%E5%88%B6)
  - [2. AI系统的不可解释性](#2-ai%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%8D%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7)
  - [3. 逻辑与直觉的割裂](#3-%E9%80%BB%E8%BE%91%E4%B8%8E%E7%9B%B4%E8%A7%89%E7%9A%84%E5%89%B2%E8%A3%82)
- [系统概述](#%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0)
  - [核心技术方案](#%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88)
    - [1. 幂等函数化的LLM调用](#1-%E5%B9%82%E7%AD%89%E5%87%BD%E6%95%B0%E5%8C%96%E7%9A%84llm%E8%B0%83%E7%94%A8)
    - [2. 本体空间的构建与维护](#2-%E6%9C%AC%E4%BD%93%E7%A9%BA%E9%97%B4%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%8E%E7%BB%B4%E6%8A%A4)
    - [3. 递归任务分解与逻辑构建](#3-%E9%80%92%E5%BD%92%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3%E4%B8%8E%E9%80%BB%E8%BE%91%E6%9E%84%E5%BB%BA)
    - [4. 工作流持久化与Agent生成](#4-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%8C%81%E4%B9%85%E5%8C%96%E4%B8%8Eagent%E7%94%9F%E6%88%90)
  - [系统架构](#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84)
    - [全局组件](#%E5%85%A8%E5%B1%80%E7%BB%84%E4%BB%B6)
    - [任务特定变量](#%E4%BB%BB%E5%8A%A1%E7%89%B9%E5%AE%9A%E5%8F%98%E9%87%8F)
  - [时序描述](#%E6%97%B6%E5%BA%8F%E6%8F%8F%E8%BF%B0)
- [长远展望：自我进化的LogicAI](#%E9%95%BF%E8%BF%9C%E5%B1%95%E6%9C%9B%E8%87%AA%E6%88%91%E8%BF%9B%E5%8C%96%E7%9A%84logicai)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

---
# 项目概述

本项目旨在构建一个基于逻辑推理的自动任务分解系统，通过将大语言模型（LLM）的调用视为幂等函数，实现复杂任务的分解与编排。系统的核心在于将LLM的直觉式推理能力与形式逻辑体系相结合，通过构建和维护一个本体空间，实现任务分解和编排的持续迭代。

在这个框架中，LLM的输入输出都被视为结构化数据，提示词的构成也遵循这一原则，不再视其为语义，而是逻辑维度的集合。例如，当我们改进提示词，要求"更通俗易懂"或"更专业"地书写时，这些要求被LogicAI视为在输入数据中增加了新的维度——具体体现为在结构化数据中增加"可读性级别"和"专业程度"等条目。这些属性名可以通过LLM来动态获取，相当于逻辑学中的类型升阶——就像问"苹果是什么？"->"苹果是水果"，进一步问"水果是什么？"->"水果是植物产品"，递归获得完整的分类链条：苹果→水果→植物产品→生物制品→物质。这种结构化的视角使系统能够引入逻辑系统来处理和优化系统工作流。

整个系统将基于逻辑的形式系统与基于神经元的LLM(视为直觉)相结合，把系统复杂度从LLM层面转移到架构层面，让每个进入LLM的子任务保持在可控的复杂度范围内，从而提高系统可应对的复杂度--将整个系统视为一个输入/输出，可视其为提升了LLM能力的一个"外部增强模型"。

# 概念解释

为更好理解本项目的设计理念，我们可以将其类比为生产线设计的知识体系：

## 生产线设计类比：

想象一个工程师需要设计一条生产线来制造特定产品。在这个类比中：

- **工程师的知识体系** = 本体空间（类似Prolog知识库）
- **具体的生产线** = 工作流
- **生产线蓝图的存储** = Agent
- **生产线操作工人的操作手册** = 工作流使用者所需的知识

工程师脑中拥有关于工业设计、流程优化、设备选型等专业知识，这些知识构成了一个逻辑体系，能够回答"如何设计一条高效的生产线"这样的问题。当工程师运用这些知识设计出具体的生产线后，生产线操作工人只需要掌握操作手册即可，无需理解整个设计知识体系。

类似地，想象一个具备充分知识的Prolog程序，它能够基于逻辑规则自主"设计"出生产线。在我们的系统中：

### 设计层知识（本体空间）：

以茶几花卉选择为例：设计师在花卉市场通过"尺寸"、"颜色"、"维护性"等维度筛选合适花卉。对应地，我们可构建提示词让LLM来选择，通过逐步添加"尺寸为XXX"、"颜色为XXX"等约束条件，最终使LLM决策与设计师判断趋同。

这一例子展示了LogicAI的核心理念：将提示词从语义表达重新定义为维度组合的结构化数据。这些维度并非任意确定，而是从交付目标（花卉）需满足的现实约束（茶几适配性、用户偏好）出发，通过逻辑推演得出。

关键在于将逻辑体系构建任务明确分割：通过专门设计的元提示词，让LLM系统性地挖掘决策维度，自动推导约束条件与逻辑规则，最终构建完整的规则库。

### 执行层知识（工作流使用）：

对于住户而言，只要花卉能带来心情愉悦即可，无需掌握前述的设计层知识。换言之，固化后的工作流(Agent)能否满足需求，关键在于功能实现，而非实现方式——无论是人工设计还是AI自主生成。

然而，当住户因失眠希望通过花卉获得心情平静时，现有Agent需要改进。这种改进依赖设计层知识，需要Agent设计者更新系统。因此，当前上下文工程设计的Agent仅适用于应用场景相对稳定的情况，而在小说创作、编程开发等变化频繁的现实领域，难以满足动态需求。

进一步思考，如现实世界中，若花卉仅是住户为其用户提供产品的一个环节，住户本身也成为设计师，同样可采用上述方法处理。这构成了工作流之间的嵌套调用关系——当然，当前版本尚不支持此特性。


## 与Chain of Thought（COT）的区别：

- COT方法：通过"step by step"的提示词引导LLM进行线性思考
- 本方法：首先通过询问"实现XXX交付物的最佳实践步骤"获取初始工作流（利用LLM直觉），然后将工作流每个步骤的输入输出进行类型分析，构建分类树，递归探究其处理规则，形成逻辑世界。通过查询这个逻辑世界来分析和改进工作流，在此过程中，逻辑世界（本体空间）和工作流实现同步进化。

##  与Reasoning + Acting（ReAct）的区别：

- ReAct方法：ReAct方法采用推理(Reasoning) + 行动(Acting) + 观察(Observation)的AI问题解决模式，但其推理环节依赖人工设计，缺乏系统化的设计方法论。
- 本方法：本方法专注于推理环节的优化，通过算法化手段替代人工设计，实现推理过程的自动化构建。

## 与Plan-and-Execute的区别:

- Plan-and-Execute是LangChain框架中的经典模式，将复杂任务分解为规划和执行两个独立阶段，面临抽象层次与执行精度间的矛盾——高层规划可能丢失关键上下文，详细规划则可能牺牲系统灵活性。当前的claude-code、Cursor等自动开发工具的实践表明了这一挑战：即使将抽象层级聚焦在软件开发领域，仍会遭遇上下文丢失导致的代码架构不一致、模块接口冲突等问题。
- 本方法采用不同的设计思路：将LLM调用建模为幂等函数，通过逻辑系统维护输入输出间的维度关系，理论上可以在保持上下文完整性的同时确保系统灵活性，有望避免抽象与执行脱节的问题，使完整小说创作、大型软件开发等超大规模任务成为可能——参考下文的长远展望，了解更多可能性。

## 与RAG的区别:

- RAG是通过向量检索外部文档来增强生成的技术架构，依赖语义相似性匹配，面临检索精度与语义理解间的矛盾----向量检索可能返回语义相关但实际无用的内容，关键词检索则可能遗漏语义相关信息。当前的企业知识问答、文档分析等RAG应用实践表明了这一挑战：即使使用先进的向量数据库，仍会遭遇语义漂移导致的检索不准确、上下文理解偏差等问题。
- 本方法采用不同的设计思路：将文档预处理为结构化维度数据存入关系数据库，通过提示词先识别问题所需的信息维度（此问题类似上文逻辑升阶--可由人或AI完成），再根据这些维度构建精确的SQL查询语句，理论上可以在保持检索精度的同时确保信息完整性，有望避免语义检索的模糊性问题，使复杂条件查询、多维度信息匹配等精确检索任务成为可能----当维度拆分足够细致时，甚至可以完全依靠关系查询替代向量检索，实现更高质量的信息匹配。这种方法可以作为独立的文档检索技术使用，核心区别在于维度体系的维护方式：可以由领域专家手工定义，也可以通过AI系统自动学习和优化。


# 要解决的问题

## 1. LLM应用的复杂度控制

现有LLM在复杂任务中存在准确率下降、逻辑混乱等问题，而提示工程等优化方法仍受限于模型能力边界。

## 2. AI系统的不可解释性

大多数基于LLM的系统缺乏清晰的推理路径，难以追溯决策过程，这在需要高可靠性的应用场景中成为关键障碍。

## 3. 逻辑与直觉的割裂

LLM的直觉式推理与专家系统的形式逻辑推理缺乏有效桥梁。直觉推理擅长模式识别，逻辑推理确保严密性，但两者难以融合，限制了AI在复杂场景中的应用。



# 系统概述

当前版本假定委托 LogicAI 执行大型任务，例如撰写长篇小说或开发项目。所有用户交互固定在单一任务中，目标是完成指定的交付物。

## 核心技术方案

### 1. 幂等函数化的LLM调用

将每次LLM调用封装为幂等函数，确保相同输入总是产生相同输出，使得系统行为可预测、可重现。这种设计让LLM调用变得类似传统程序中的纯函数，便于组合和测试。

### 2. 本体空间的构建与维护

系统维护一个类似Prolog知识库的逻辑空间，该空间包含设计工作流所需的核心知识：

- **知识树(分类树)**：将所有输入输出进入分类树，形成层次化的类型体系
- **逻辑规则库**：每种类型数据的处理规则、约束条件和转换规律
- **推理引擎**：能够基于已有规则推导出新的工作流设计方案
- **知识更新机制**：从工作流执行结果中提取新的规则和模式

### 3. 递归任务分解与逻辑构建

通过以下步骤实现从直觉到逻辑的进化：

1. **初始直觉获取**：向LLM询问最佳实践，获得初始工作流
2. **类型分析**：分析每个步骤的输入输出，构建类型分类树
3. **规则提取**：递归探究各类型的处理规律，形成逻辑规则
4. **逻辑查询**：通过查询逻辑世界分析工作流的合理性
5. **同步优化**：基于分析结果同时改进工作流和逻辑规则

### 4. 工作流持久化与Agent生成

将验证过的工作流持久化为Agent，每个Agent专注于解决特定类型的问题。使用者无需理解底层的设计逻辑，只需掌握Agent的使用方法即可。

## 系统架构

### 全局组件
LogicAI 在所有任务和交互中维护以下全局组件：

1. **本体空间（RDF 格式）**：所有任务共享的 RDF 格式本体空间，存储运行中产生的概念、类属关系、概念层的约束、技巧、处理过程等。
2. **内置有限状态机（FSM）**：
   - **规划 FSM**：分析和分解交付物，创建和启动子 FSM。
   - **指代消解 FSM**：维护 RDF 中的同义词和逻辑隐喻。通过DL reasoner的**等价性检查(Equivalence Checking)**，**实例检查(Instance Checking)** 来关联相同实体。
1. **推理增强**：利用Apache Jena等DL推理器执行**一致性检查 (Consistency Checking)** 和 **概念可满足性(Concept Satisfiability)** 验证，以确定FSM中Actin(函数)的输入输出集合。通过SWI-Prolog的semweb库实现基于OWL的自定义约束机制，针对本体中的形容词和副词构建专用约束规则，并基于这些约束对FSM的状态转换及其对应的输入输出进行验证和修改。

### 任务特定变量
每个任务初始化以下变量：

1. **交付物知识图谱**：RDF 格式的知识图谱，存储描述交付物要求、约束和限定的知识，存储在此任务的命名图（Named Graph）中。
2. **任务 FSM**：负责具体任务执行的嵌套 FSM，由内置 FSM 创建，在适当时候执行。

## 时序描述
LogicAI 的运行过程通过以下时序描述，展示从用户输入到任务执行的流程(当前版本会暂缓实现增强推理，因此本体空间的准确度依赖LLM，先实现初级目标后再添加增强推理支持)：

1. 接收用户输入语句，可能包含交付物需求或处理过程描述。
2. 指代消解 FSM 查询本体空间（RDF），解析同义词和逻辑隐喻，重新表达输入以消除歧义。
3. 规划 FSM 执行以下操作：
   - 将输入转换为 RDF，更新交付物知识图谱，包含交付物或子交付物的要求、约束和限定（可想象为无模式对象)。如输入涉及处理过程（例如子 FSM 的步骤或流程），也在这里处理并调整相应的子FSM。
   - 根据交付物知识图，规划交付物制作过程，生成或更新任务 FSM。
   - 构建交付物知识图和本体空间的联系，如果没有则新建，包括递归询问LLM构建相关规则--重点是完备性规则。
   - 倒序遍历FSM的状态，对FSM中每个有动作的状态，通过SPARQL查询其完备性规则，并检查输入与输出之间的完备性--如果不完备，添加输入，并递归使用规划FSM更新FSM。
   - 对每个状态的输出，递归执行此规划FSM，确保此动作的复杂度足够低--通过递归拆解的方式。
   - 为任务FSM添加容错及进度报告。
1. 执行任务FSM--这可能是一个长周期期动作，通过FSM的持久化来暂停/恢复任务。
2. 向用户返回任务进展或交付物输出。


# 长远展望：自我进化的LogicAI

**核心洞察**：在我们的设计框架下，一切皆函数，一切皆工作流。目标判定本身、工作流设计过程、系统优化决策等，都可以被建模为函数，因此都可以被纳入本方法的持续迭代改进范围。

**自举基础环境**：我们提供的初始系统相当于一个具备自我更新能力的不动点——类似于数学中的巴拿赫不动点定理所描述的稳定状态，系统在这个基础上可以通过应用自身的逻辑推理能力来改进自己的推理能力。这种递归式的自我改进机制使得LogicAI能够在有限的初始投入下实现无限的成长潜力。

**元级工作流**：系统不仅能够分解用户任务，还能够分解"如何更好地分解任务"这一元任务，从而实现对自身能力的反思与提升。本体空间的逻辑规则也将包含关于知识获取、规则优化、推理改进等元认知规则。

**无边界成长**：通过这种设计，LogicAI理论上能够突破初始设计者的认知边界，在实践中发现更优的问题解决模式，并将这些发现编码为新的逻辑规则，形成真正的人工智能自主进化。

**当前阶段聚焦**：尽管长远目标如此宏大，限于精力和现实约束，当前版本将专注于复杂任务的自动拆解和实用Agent的生成。我相信，即使是这个相对有限的目标，也将为构建更强大的自进化AI系统奠定基础。

这一方法的目标是实现从LLM直觉向形式逻辑的进化，构建一个能够像人类工程师一样思考和设计的智能系统，并为未来的自主进化预留可能性。
